import os
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import scipy.stats as stats
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split

# ====================================================
# Configura√ß√µes e Ajustes de Display
# ====================================================
# Caminho da pasta com os CSVs
pasta = os.path.dirname(os.path.abspath(__file__))
 # Ajuste se necess√°rio
# Configura√ß√µes para exibi√ß√£o tabular truncada com ellipsis
pd.set_option('display.max_rows', 20)
pd.set_option('display.max_columns', 20)
pd.set_option('display.width', 1000)

# ====================================================
# Fun√ß√£o: Carregar e Concatenar Dados
# ====================================================
def carregar_dados(pasta):
    """
    L√™ todos os arquivos .csv da pasta, trata 'm' como NaN,
    e retorna um DataFrame √∫nico.
    """
    arquivos = [f for f in os.listdir(pasta) if f.lower().endswith('.csv')]
    lista_df = []

    for arquivo in sorted(arquivos):
        caminho = os.path.join(pasta, arquivo)
        try:
            df = pd.read_csv(caminho, na_values='m')
            df['Ano'] = int(arquivo[:4])
            lista_df.append(df)
            print(f"‚úÖ {arquivo}: {df.shape[0]} linhas, {df.shape[1]} colunas")
        except Exception as e:
            print(f"‚ùå Erro em {arquivo}: {e}")

    if not lista_df:
        raise RuntimeError("Nenhum CSV carregado com sucesso.")

    df_total = pd.concat(lista_df, ignore_index=True)
    print(f"\nüì¶ Total combinado: {df_total.shape[0]} linhas, {df_total.shape[1]} colunas")
    return df_total

# ====================================================
# Fun√ß√£o: Pr√©-processamento
# ====================================================
def preprocessar(df):
    """
    Imputa NAs pela mediana e garante tipos corretos.
    """
    imputer = SimpleImputer(strategy='median')
    num_cols = df.select_dtypes(include=[np.number]).columns
    df[num_cols] = imputer.fit_transform(df[num_cols])

    for col in ['Country', 'S', 'Ano']:
        if col in df.columns:
            df[col] = df[col].astype('category')
    return df

# ====================================================
# Fun√ß√£o: Estat√≠sticas Descritivas
# ====================================================
def estatisticas(df, grupo=None):
    """
    Retorna DataFrame de estat√≠sticas descritivas.
    Se grupo for fornecido, agrupa antes de resumir.
    Para agrupamentos, empilha vari√°veis para reduzir n√∫mero de colunas,
    deixando m√©tricas (count, mean, std, min, 25%, 50%, 75%, max) como colunas.
    """
    if grupo:
        # Estat√≠sticas por grupo: resultado com colunas MultiIndex (var, stat)
        # Adicionando observed=True para resolver o aviso
        descr = df.groupby(grupo, observed=True).describe()
        # Empilha n√≠vel de vari√°vel, mantendo estat√≠sticas como colunas
        # Adicionando future_stack=True para resolver o aviso
        descr = descr.stack(level=0, future_stack=True)
        # Renomeia √≠ndices para ficar claro
        descr.index.names = list(descr.index.names)  # ex: ['S','variable'] or ['S','Country','variable']
        return descr
    else:
        # Estat√≠sticas gerais: m√©tricas como colunas, vari√°veis como √≠ndice
        descr = df.describe().T
        return descr

# ====================================================
# Fun√ß√£o: Teste Normalidade e Intervalo de Confian√ßa
# ====================================================
def teste_normalidade_e_ic(df, col, conf=0.95):
    dados = df[col].dropna()
    stat, p = stats.shapiro(dados)
    interpret = (f"A vari√°vel {col} parece seguir uma distribui√ß√£o normal" if p > 0.05
                 else f"A vari√°vel {col} N√ÉO segue distribui√ß√£o normal")
    interpret += f" (p = {p:.3e})."

    n = len(dados)
    m = np.mean(dados)
    se = stats.sem(dados)
    margem = se * stats.t.ppf((1 + conf) / 2., n - 1)
    ic_lower, ic_upper = m - margem, m + margem
    ic_text = (f"Intervalo de confian√ßa de {conf*100:.0f}% para a m√©dia de {col}: "
               f"[{ic_lower:.4f}, {ic_upper:.4f}].")
    return interpret, ic_text

# ====================================================
# Fun√ß√µes de Gr√°ficos
# ====================================================
def plot_heatmap(df):
    plt.figure(figsize=(12, 10))
    sns.heatmap(df.select_dtypes(include=[np.number]).corr(),
                cmap='coolwarm', center=0)
    plt.title('Heatmap de Correla√ß√£o')
    plt.show()

def plot_scatter_matrix(df, cols, hue='S'):
    sample = df.sample(n=min(200, len(df)), random_state=42)
    sns.pairplot(sample[cols + [hue]], hue=hue)
    plt.show()

# ====================================================
# Fun√ß√£o: Import√¢ncia de Atributos
# ====================================================
def atributos_importantes(df, target='S', top_n=10):
    X = df.drop(columns=[target, 'Country', 'Ano', 'Num'], errors='ignore')
    X = X.select_dtypes(include=[np.number])
    y = df[target].cat.codes
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=0)
    clf = DecisionTreeClassifier(random_state=0)
    clf.fit(X_train, y_train)
    importancias = pd.Series(clf.feature_importances_, index=X.columns)
    return importancias.sort_values(ascending=False).head(top_n)

# ====================================================
# Rotina Principal
# ====================================================
if __name__ == '__main__':
    df = carregar_dados(pasta)
    df = preprocessar(df)

    if 'X1' in df.columns:
        df = df.drop(columns=['X1'])

    print("\n" + "="*60)
    print("üìä ESTAT√çSTICAS GERAIS")
    print("="*60)
    print(estatisticas(df))

    print("\n" + "="*60)
    print("üìä ESTAT√çSTICAS POR SETOR (S)")
    print("="*60)
    print(estatisticas(df, grupo='S'))

    print("\n" + "="*60)
    print("üî• HEATMAP DE CORRELA√á√ÉO")
    print("="*60)
    plt.figure(figsize=(12, 10))
    sns.heatmap(df.select_dtypes(include=[np.number]).corr(),
                cmap='coolwarm', center=0)
    plt.title('Heatmap de Correla√ß√£o')
    plt.tight_layout()
    plt.savefig("heatmap_correlacao.png")
    plt.close()
    print("üì∏ Guardado como: heatmap_correlacao.png")

    print("\n" + "="*60)
    print("üì∑ SCATTERPLOT MATRIX (X2 a X5)")
    print("="*60)
    sample = df.sample(n=min(200, len(df)), random_state=42)
    sns.pairplot(sample[['X2', 'X3', 'X4', 'X5', 'S']], hue='S')
    plt.savefig("scatterplot_matrix.png")
    plt.close()
    print("üì∏ Guardado como: scatterplot_matrix.png")

    print("\n" + "="*60)
    print("‚≠ê TOP 10 ATRIBUTOS MAIS IMPORTANTES")
    print("="*60)
    top_atributos = atributos_importantes(df)
    print(top_atributos)

    # Gr√°fico dos atributos mais importantes
    top_atributos.plot(kind='barh', title='Top 10 Atributos Mais Importantes')
    plt.xlabel("Import√¢ncia")
    plt.tight_layout()
    plt.savefig("grafico_top_atributos.png")
    plt.close()
    print("üì∏ Guardado como: grafico_top_atributos.png")

    print("\nüìå CONCLUS√ÉO MANUAL (exemplo):")
    print("- A maioria das vari√°veis apresenta distribui√ß√£o assim√©trica.")
    print("- Existem fortes correla√ß√µes entre certas vari√°veis (ver heatmap).")
    print("- X50, X47 e X24 s√£o os atributos mais relevantes para o target 'S'.")

    print("\n‚úÖ An√°lise estat√≠stica e visual finalizada com sucesso!")
